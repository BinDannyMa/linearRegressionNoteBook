{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression - Bayesian Approach\n",
    "\n",
    "\n",
    "<font color=red size=3> **Application Scenario:**</font> \n",
    "The Bayesian Polynomial Regression or Regularized Polynomial Regression is suitable for the following scenario.\n",
    "- Relationship between input (features) and output (target): Non-Linear\n",
    "- The Polynomial Regression model is too complex and overfits the data (high variance)\n",
    "- We need a complex model but want to \"turn-off\" the unnecessary polynomial features. In other words, we want to regularize the complex model.\n",
    "\n",
    "\n",
    "\n",
    "## Choosing Model Complexity in High-Degree Polynomial Regression\n",
    "\n",
    "There are two approaches to make an optimal tradeoff between overfitting (high variance) and underfitting (high bias).\n",
    "- <font color=red>Frequentist Learning (MLE)</font>: \n",
    "   \n",
    "   -- Using cross-validation determine the optimal degree (model complexity) that produces best generalization\n",
    "   \n",
    "   -- Using learning curve determine the optimal degree (model complexity) that produces best generalization\n",
    "   \n",
    "- <font color=red>Bayesian Learning (MAP or regularized regression)</font>: \n",
    "   \n",
    "   -- Using cross-validation learn the optimal regularization (penalty) coefficients that produce best generalization\n",
    "\n",
    "\n",
    "##### In this notebook we implement the Bayesian approach.\n",
    "We perform **regularized polynomial linear regression** using the sklearn's **regularized OLS Linear Regression** model.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston\n",
    "\n",
    "This dataset provides housing values in suburbs of Boston.\n",
    "\n",
    "The **MEDV** variable is the target variable.\n",
    "\n",
    "### Data description\n",
    "\n",
    "The Boston data frame has 506 rows and 14 columns.\n",
    "\n",
    "This data frame contains the following columns:\n",
    "\n",
    "- CRIM: per capita crime rate by town.\n",
    "\n",
    "- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "- INDUS: proportion of non-retail business acres per town.\n",
    "\n",
    "- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "- NOX: nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "- RM: average number of rooms per dwelling.\n",
    "\n",
    "- AGE: proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "- DIS: weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "- RAD: index of accessibility to radial highways.\n",
    "\n",
    "- TAX: full-value property-tax rate per $10,000.\n",
    "\n",
    "- PTRATIO: pupil-teacher ratio by town.\n",
    "\n",
    "- B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "- LSTAT: lower status of the population (percent).\n",
    "\n",
    "- MEDV: median value of owner-occupied homes in $1000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First load the data and explore the feature names, target names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "boston = datasets.load_boston()\n",
    "print(boston.data.shape, boston.target.shape)\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A DataFrame Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "df = pd.concat([df,pd.Series(boston.target,name='MEDV')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Check of the Data\n",
    "\n",
    "Let’s take a look at the top five rows using the DataFrame’s head() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Data\n",
    "\n",
    "DataFrame’s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null float64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null float64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "MEDV       506 non-null float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data: Describe Numerical Attributes\n",
    "\n",
    "DataFrame's describe() method shows a summary of the numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Separate Feature Set (Data Matrix X) and Target (1D Vector y)\n",
    "\n",
    "Create a data matrix (X) that contains all features and a 1D target vector (y) containing the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# It contains all attributes (all features and the target)\n",
    "allData = df\n",
    "\n",
    "\n",
    "# Use the following code to select ALL features\n",
    "X = df.drop(columns='MEDV')  # Data Matrix containing all features excluding the target\n",
    "\n",
    "\n",
    "y = df['MEDV'] # 1D targer vector\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Scale The Features\n",
    "\n",
    "We should ensure that all features have a similar scale. Otherwise optimization algorithms (e.g., Gradient Descent based algorithms) will take much longer time to converge.\n",
    "\n",
    "Also, regularization techniques are sensitive to the scale of data. Thus, we must scale the features before applying regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Learning Approach for Bias-Variance Tradeoff (Choosing Model Complexity)\n",
    "\n",
    "\n",
    "Unlike the Frequentist approach (MLE) that is used to determine optimal degree of the polynomial (how complex the model should be), the Bayesian approach enables us to determine the optimal regularization (penalty) coefficients that produce best generalization.\n",
    "\n",
    "Recall that MLE pics the polynomial degree that is the best for modeling the training data. If the data is noisy (input and out has non-linear relationship), a high-degree polynomial is requred that results in a complex function. Such complex models are prone to overfitting (shows high variance).\n",
    "\n",
    "The Bayesian approach (Maximum a posteriori or MAP) is used to resolve this issue by encouraging the parameters to be small, thus resulting in a smoother curve. This is done by a technique called **regularization**. Regularization can “turn-off” some features that don’t play any role explaining the variation in our prediction. In polynomial regression, it kills the non-contributing polynomial features.\n",
    "\n",
    "Regularization allows complex models to be trained on data sets of limited size without severe overfitting, essentially by limiting the effective model complexity. We can afford a complex model without experiencing overfitting.\n",
    "\n",
    "\n",
    "\n",
    "### Regularized (Penalized) Regression\n",
    "\n",
    "\n",
    "In regularized Regression we regularize (penalize) the model parameters that allow complex models (high-degree Polynomial Regression model) to be trained on datasets of limited size without severe overfitting. It essentially limits the effective model complexity. \n",
    "\n",
    "Thus, the problem of determining the optimal model complexity (degree of polynomial or the basis function) is shifted from one of finding the appropriate number of basis functions (that we did in the Frequentist approach) to one of determining a suitable value of the regularization coefficient.\n",
    "\n",
    "A model with some regularization typically performs better than a model without any regularization. Thus, we should generally prefer regularized Regression over plain Linear Regression.\n",
    "\n",
    "Moreover, the OLS method (Normal Equation) requires computing the inverse of a matrix, but that matrix is not always invertible. In contrast, the matrix for regularized Regression (e.g., Ridge Regression) is always invertible.\n",
    "\n",
    "To implement the Bayesian approach, we will train a high-degree polynomial model that might have high-variance (suffers from overfitting). Then, we will reduce overfitting via regularization. First, we determine the optimal polynomial degree by using MLE and then reduce its overfitting.\n",
    "- Step 1: Using MLE determine the optimal degree of the high-degree polynomial.\n",
    "- Step 2: Then, apply the regularized OLS method (MAP) and determine the optimal value for the regularization parameter via hyperparameter tuning. We will see that due to regularization the model will be better generalizable by weaking some of the features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Using MLE Determine the Optimal Degree of the Polynomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Degree:  2\n"
     ]
    }
   ],
   "source": [
    "degreeList = [1,2,3,4]\n",
    "\n",
    "mse_train, mse_test = [], []\n",
    "\n",
    "for degree in degreeList:\n",
    "\n",
    "    model = make_pipeline(PolynomialFeatures(degree, include_bias=False), StandardScaler(), LinearRegression()) \n",
    "  \n",
    "    model.fit(X_train, y_train)\n",
    "       \n",
    "    # Make prediction \n",
    "    y_train_predicted = model.predict(X_train)\n",
    "    y_test_predicted = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    mse_train.append(mean_squared_error(y_train, y_train_predicted))\n",
    "    mse_test.append(mean_squared_error(y_test, y_test_predicted))\n",
    "   \n",
    "\n",
    "plt.figure(figsize=(10, 6))   \n",
    "plt.plot(degreeList, np.sqrt(mse_test), \"ro-\", alpha=1.0, linewidth=1.0, label=\"Test RMSE\")\n",
    "plt.plot(degreeList, np.sqrt(mse_train), \"bo-\", alpha=1.0, linewidth=1.0, label=\"Train RMSE\") \n",
    "plt.legend(loc=\"best\", fontsize=14) \n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"RMSE for Varying Degree\")\n",
    "plt.show()\n",
    "\n",
    "# Find the value of optimal degree for the polynomial that gives smallest RMSE\n",
    "\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "j = 0\n",
    "min_rmse = rmse_test[j]\n",
    "optimal_degree = 1\n",
    "\n",
    "for i in degreeList:\n",
    "    if(rmse_test[j] < min_rmse):\n",
    "        min_rmse = rmse_test[j]\n",
    "        optimal_degree = i\n",
    "    j +=1\n",
    "    \n",
    "print(\"\\nOptimal Degree: \", optimal_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Apply the Regularized OLS Method (MAP) on the Optimal Polynomial Model\n",
    "\n",
    "\n",
    "There are three different regularized OLS models.\n",
    "\n",
    "- Ridge Regression ($l_2$ norm)\n",
    "- Lasso Regression ($l_1$ norm)\n",
    "- Elastic Net (it combines $l_1$ and $l_2$ priors as regularizer)\n",
    "\n",
    "We will use the OLS Ridge and Lasso Regression algorithms on the high-degree polynomial dataset.\n",
    "\n",
    "\n",
    "#### Using cross-validation we determine the optimal regularization (penalty) coefficients that produce best generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "We will implement the Ridge Regression with Scikit-Learn using a closed-form solution. It uses a matrix factorization technique by André-Louis **Cholesky**. However, there are other solvers that we can choose from.\n",
    "\n",
    "We need to find optimal values for the following **two hyperparameters** of the Ridge regression model.\n",
    "\n",
    "- alpha : {float, array-like}, shape (n_targets)\n",
    "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization.\n",
    "\n",
    "- solver : {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}\n",
    "\n",
    "The solvers:\n",
    "1. ‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than ‘cholesky’.\n",
    "2. ‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution.\n",
    "3. ‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set tol and max_iter).\n",
    "4. ‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative procedure.\n",
    "5. ‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses its improved, unbiased version named SAGA. Both methods also use an iterative procedure, and are often faster than other solvers when both n_samples and n_features are large. Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.\n",
    "6. ‘auto’ chooses the solver automatically based on the type of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Hyperparameter Tunining\n",
    "\n",
    "\n",
    "We need to select the best model based on the optimal values of these hyperparameters. This process is called hyperparameter tuning.\n",
    "\n",
    "The best way to do hyperparameter tuning is to use **cross-validation**.\n",
    "\n",
    "We will use Scikit-Learn’s GridSearchCV to search the combinations of hyperparameter values that provide best performance.\n",
    "\n",
    "We need to tell which hyperparameters we want the GridSearchCV to experiment with, and what values to try out. It will evaluate all the possible combinations of hyperparameter values, using cross-validation. \n",
    "\n",
    "\n",
    "### Important:\n",
    "\n",
    "The GridSearchCV takes an argument to define the scoring metric (performance measure). \n",
    "\n",
    "See the list of possible scoring functions:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "For regression, we may use \"neg_mean_squared_error\" or \"explained_variance\" scoring function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a High-Degree Polynomial Data Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Original Features:  13\n",
      "No. of Augmented Features:  104\n"
     ]
    }
   ],
   "source": [
    "# Variable that speficies the degree of the polynomial to be added to the feature vector\n",
    "poly_degree = optimal_degree\n",
    "\n",
    "\n",
    "# Add polynomial terms with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "\n",
    "\n",
    "print(\"No. of Original Features: \", X_train.shape[1])\n",
    "print(\"No. of Augmented Features: \", X_train_poly.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Via Grid Search: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 120 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (negative mean squared error): -13.650130\n",
      "Optimal Hyperparameter Values:  {'alpha': 0.01, 'solver': 'saga'}\n",
      "\n",
      "\n",
      "CPU times: user 1.36 s, sys: 133 ms, total: 1.49 s\n",
      "Wall time: 9.83 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:    9.7s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The param_grid tells Scikit-Learn to evaluate all combinations of the hyperparameter values\n",
    "\n",
    "param_grid = {'alpha': np.linspace(0.01, 1.0, num=20), \n",
    "              'solver': [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"saga\"]}\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=-1)\n",
    "ridge_cv.fit(X_train_poly, y_train)\n",
    "\n",
    "\n",
    "params_optimal_ridge = ridge_cv.best_params_\n",
    "\n",
    "print(\"Best Score (negative mean squared error): %f\" % ridge_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_ridge)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select The Best Model for the Ridge Regressor\n",
    "\n",
    "Using the optimal hyperparameter values, create the best model.\n",
    "Then, fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha:  0.01\n",
      "Optimal alpha:  saga\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 8.39\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "\n",
    "# Optimal model parameters\n",
    "ridge_alpha = ridge_cv.best_params_['alpha']\n",
    "print(\"Optimal alpha: \", ridge_alpha)\n",
    "\n",
    "ridge_solver = ridge_cv.best_params_['solver']\n",
    "print(\"Optimal alpha: \", ridge_solver)\n",
    "\n",
    "\n",
    "# Create Ridge linear regression object\n",
    "lin_reg_ridge = Ridge(alpha=ridge_alpha, solver=ridge_solver)\n",
    "\n",
    "# Train the model\n",
    "lin_reg_ridge.fit(X_train_poly, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_ridge = lin_reg_ridge.predict(X_train_poly)\n",
    "\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_ridge))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Using Test Data: Ridge Regression\n",
    "\n",
    "We will use the optimal degree for the polynomial to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------- Test Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 12.93\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Add polynomial and bias term with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Test Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_test_poly_predicted = lin_reg_ridge.predict(X_test_poly)\n",
    "\n",
    "\n",
    "ridge_test_mse = mean_squared_error(y_test, y_test_poly_predicted)\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\" % ridge_test_mse)\n",
    "\n",
    "\n",
    "\n",
    "ridge_test_r2_score = r2_score(y_test, y_test_poly_predicted)\n",
    "print('Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f' \n",
    "      % ridge_test_r2_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a High-Degree Polynomial Data Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable that speficies the degree of the polynomial to be added to the feature vector\n",
    "poly_degree = optimal_degree\n",
    "\n",
    "\n",
    "# Add polynomial terms with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Via Grid Search: Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (negative mean squared error): -12.208312\n",
      "Optimal Hyperparameter Values:  {'alpha': 15.510204081632654}\n",
      "\n",
      "\n",
      "CPU times: user 529 ms, sys: 182 ms, total: 711 ms\n",
      "Wall time: 576 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The param_grid tells Scikit-Learn to evaluate all combinations of the hyperparameter values\n",
    "\n",
    "#param_grid = {'alpha': np.linspace(10.0, 20.0, num=10)}\n",
    "param_grid = {'alpha': np.linspace(10.0, 20.0)}\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "lasso_cv = GridSearchCV(ridge, param_grid, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=-1)\n",
    "lasso_cv.fit(X_train_poly, y_train)\n",
    "\n",
    "params_optimal_lasso = lasso_cv.best_params_\n",
    "\n",
    "print(\"Best Score (negative mean squared error): %f\" % lasso_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_lasso)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select The Best Model for the Lasso Regressor\n",
    "\n",
    "Using the optimal hyperparameter values, create the best model.\n",
    "Then, fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha:  15.510204081632654\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 6.57\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "\n",
    "# Optimal model parameters\n",
    "lasso_alpha = lasso_cv.best_params_['alpha']\n",
    "print(\"Optimal alpha: \", lasso_alpha)\n",
    "\n",
    "\n",
    "# Create Lasso linear regression object\n",
    "lin_reg_lasso = Ridge(alpha=lasso_alpha)\n",
    "\n",
    "# Train the model\n",
    "lin_reg_lasso.fit(X_train_poly, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_lasso = lin_reg_lasso.predict(X_train_poly)\n",
    "\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_lasso))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" \n",
    "      % r2_score(y_train, y_train_predicted_lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Using Test Data: Lasso Regression\n",
    "\n",
    "We will use the optimal degree for the polynomial to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------- Test Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 13.29\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Add polynomial and bias term with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Test Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_test_poly_predicted = lin_reg_lasso.predict(X_test_poly)\n",
    "\n",
    "lasso_test_mse = mean_squared_error(y_test, y_test_poly_predicted)\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\" % lasso_test_mse)\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "\n",
    "lasso_test_r2_score = r2_score(y_test, y_test_poly_predicted)\n",
    "print('Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f' \n",
    "      % lasso_test_r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ridge Regression vs. Lasso Regression\n",
    "\n",
    "\n",
    "Lasso Regression uses an $l_1$ penalty, which tends to push the weights down to exactly zero. This leads to **<font color=red>sparse models</font>**, where all weights are zero except for the most important weights. This is a way to perform **feature selection** automatically, which is good if we suspect that only a few features actually matter. When we are not sure, we should prefer Ridge Regression.\n",
    "\n",
    "\n",
    "## Comparison of the Weight Values: Ridge Regression vs. Lasso Regression\n",
    "\n",
    "We will see that when the regularization coefficient (alpha) is large (e.g., $ \\geq 1.0$), Lasso regression tends to drive the weight values towards 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ridge (alpha = 0.010)       Lasso (alpha = 15.510)\n",
      "   Test MSE = 12.925           Test MSE = 13.292\n",
      "   Test R2 Score = 0.824       Test R2 Score = 0.819\n",
      "________________________________________________________________\n",
      "\n",
      "      -0.181449                       -0.113237\n",
      "       0.047159                        0.035266\n",
      "      -0.266529                       -0.046317\n",
      "       0.078989                        0.088411\n",
      "      -0.524234                       -0.730552\n",
      "       3.013070                        3.284779\n",
      "      -0.538919                       -0.830703\n",
      "      -0.607133                       -1.376246\n",
      "       0.213348                        0.632927\n",
      "      -0.562102                       -0.836610\n",
      "      -0.960287                       -0.759206\n",
      "       0.346323                        0.522407\n",
      "      -2.589432                       -2.936501\n",
      "       0.080660                        0.033523\n",
      "       0.067011                        0.041845\n",
      "       0.005558                        0.090824\n",
      "       0.582682                        1.688107\n",
      "      -0.220256                       -0.489835\n",
      "       0.414515                        0.772682\n",
      "       0.073301                        0.177326\n",
      "       0.334735                        0.358028\n",
      "      -0.395303                       -0.569757\n",
      "      -0.026726                        0.153634\n",
      "       0.116143                       -0.002707\n",
      "      -0.122805                       -0.162936\n",
      "      -0.111570                        0.704568\n",
      "       0.217281                        0.240889\n",
      "      -0.014175                       -0.103990\n",
      "       0.268015                        0.217521\n",
      "       0.190845                        0.167190\n",
      "       0.076334                       -0.026980\n",
      "      -0.098253                        0.000605\n",
      "      -0.014449                       -0.289750\n",
      "      -0.017608                       -0.272219\n",
      "       0.009346                        0.730931\n",
      "       0.208143                        0.198545\n",
      "      -0.036458                       -0.059280\n",
      "       0.125892                       -0.499479\n",
      "       0.491464                        0.705191\n",
      "       0.231076                        0.039027\n",
      "       0.149800                        0.627721\n",
      "      -0.057834                        0.704881\n",
      "       0.496316                        0.555312\n",
      "       0.251473                        0.732055\n",
      "       0.150518                        0.014803\n",
      "       0.198781                        0.295730\n",
      "      -0.371335                       -0.524202\n",
      "       0.026078                        0.122040\n",
      "      -0.562759                       -0.612921\n",
      "       0.268231                        0.300226\n",
      "      -0.849888                       -1.037360\n",
      "      -0.640757                       -0.754686\n",
      "       0.283960                        0.173352\n",
      "      -0.317882                       -0.500057\n",
      "       0.303711                        0.247488\n",
      "       0.538541                        0.569280\n",
      "       0.074114                       -0.127455\n",
      "      -0.121229                       -0.113298\n",
      "      -0.559662                       -0.448693\n",
      "      -0.445341                       -0.446773\n",
      "      -0.840360                       -0.797775\n",
      "      -0.017079                       -0.256530\n",
      "       0.475315                        0.701786\n",
      "      -0.424713                       -0.632777\n",
      "      -0.044892                        0.087603\n",
      "      -0.133223                       -0.469733\n",
      "      -0.157391                       -0.151340\n",
      "       0.180705                        0.502810\n",
      "       0.489478                        0.525058\n",
      "      -0.328762                       -0.676121\n",
      "       0.061606                        0.033040\n",
      "      -0.837774                       -1.153284\n",
      "      -0.595525                       -0.928456\n",
      "      -0.951045                       -1.018749\n",
      "      -0.234709                       -0.402656\n",
      "      -0.339881                       -0.071402\n",
      "       0.209662                        0.096513\n",
      "      -0.103023                        0.217506\n",
      "       0.416794                        0.861954\n",
      "       0.423690                        0.252291\n",
      "      -0.148526                        0.006792\n",
      "      -0.307788                       -0.709970\n",
      "      -0.807708                       -1.068833\n",
      "       0.289650                        0.955486\n",
      "       0.194544                       -0.019943\n",
      "       0.085621                       -0.046950\n",
      "       0.199353                       -0.124078\n",
      "      -0.209819                       -0.291661\n",
      "       0.746563                        0.689227\n",
      "      -0.142288                       -0.723392\n",
      "       0.414117                        0.703950\n",
      "       0.558448                        0.408262\n",
      "      -0.018004                       -0.025992\n",
      "      -0.731832                       -1.362199\n",
      "       0.344265                        0.235454\n",
      "       0.481832                        1.162672\n",
      "      -0.140865                       -0.423980\n",
      "      -0.769871                       -1.055615\n",
      "      -0.221686                       -0.159430\n",
      "      -0.088528                        0.076740\n",
      "       0.036916                        0.175263\n",
      "      -0.351708                       -0.312784\n",
      "      -0.476630                       -0.141113\n",
      "       0.716574                        0.960637\n"
     ]
    }
   ],
   "source": [
    "print(\"%8s (alpha = %3.3f)  %10s (alpha = %3.3f)\" % (\"Ridge\",ridge_alpha, \"Lasso\", lasso_alpha))\n",
    "print(\"%2s Test MSE = %3.3f  %8s Test MSE = %3.3f\" % (\" \",ridge_test_mse, \" \", lasso_test_mse))\n",
    "print(\"%2s Test R2 Score = %3.3f  %4s Test R2 Score = %3.3f\" % (\" \",ridge_test_r2_score, \" \", lasso_test_r2_score))\n",
    "print(\"________________________________________________________________\\n\")\n",
    "for i in range(lin_reg_ridge.coef_.shape[0]):\n",
    "    print(\"%15f  %30f\" % (lin_reg_ridge.coef_[i], lin_reg_lasso.coef_[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Note: Regularized Polynomial Regression\n",
    "\n",
    "We observe that only by increasing model complexity (higher-degree polynomial) and regularizing its weights, we could improve performance on the test data for the given dataset."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
