{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Boston Housing Dataset - Regularized Polynomial OLS\n",
    "\n",
    "We will perform **polynomial linear regression** using the sklearn's regularized OLS Linear Regression model.\n",
    "\n",
    "### Polynomial Regression\n",
    "If our data is actually more complex than a simple straight line, we can still use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.\n",
    "\n",
    "Scikit-Learn’s PolynomialFeatures class to transform our training data, adding the higher-degree polynomial of each feature in the training set as new features. PolynomialFeatures also adds all combinations of features up to the given degree.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston\n",
    "\n",
    "This dataset provides housing values in suburbs of Boston.\n",
    "\n",
    "The **MEDV** variable is the target variable.\n",
    "\n",
    "### Data description\n",
    "\n",
    "The Boston data frame has 506 rows and 14 columns.\n",
    "\n",
    "This data frame contains the following columns:\n",
    "\n",
    "- CRIM: per capita crime rate by town.\n",
    "\n",
    "- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "- INDUS: proportion of non-retail business acres per town.\n",
    "\n",
    "- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "- NOX: nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "- RM: average number of rooms per dwelling.\n",
    "\n",
    "- AGE: proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "- DIS: weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "- RAD: index of accessibility to radial highways.\n",
    "\n",
    "- TAX: full-value property-tax rate per $10,000.\n",
    "\n",
    "- PTRATIO: pupil-teacher ratio by town.\n",
    "\n",
    "- B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "- LSTAT: lower status of the population (percent).\n",
    "\n",
    "- MEDV: median value of owner-occupied homes in $1000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First load the data and explore the feature names, target names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "boston = datasets.load_boston()\n",
    "print(boston.data.shape, boston.target.shape)\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A DataFrame Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "df = pd.concat([df,pd.Series(boston.target,name='MEDV')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Check of the Data\n",
    "\n",
    "Let’s take a look at the top five rows using the DataFrame’s head() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Data\n",
    "\n",
    "DataFrame’s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null float64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null float64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "MEDV       506 non-null float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data: Describe Numerical Attributes\n",
    "\n",
    "DataFrame's describe() method shows a summary of the numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Separate Feature Set (Data Matrix X) and Target (1D Vector y)\n",
    "\n",
    "Create a data matrix (X) that contains all features and a 1D target vector (y) containing the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# It contains all attributes (all features and the target)\n",
    "allData = df\n",
    "\n",
    "# Use the following code to select a subset of the features, e.g., \"LSTAT\"\n",
    "#X = df[['LSTAT', 'RM']]\n",
    "\n",
    "\n",
    "# Use the following code to select ALL features\n",
    "X = df.drop(columns='MEDV')  # Data Matrix containing all features excluding the target\n",
    "\n",
    "\n",
    "y = df['MEDV'] # 1D targer vector\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Scale The Features\n",
    "\n",
    "We should ensure that all features have a similar scale. Otherwise optimization algorithms (e.g., Gradient Descent based algorithms) will take much longer time to converge.\n",
    "\n",
    "Also, regularization techniques are sensitive to the scale of data. Thus, we must scale the features before applying regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Model Complexity in High-Degree Polynomial Regression\n",
    "\n",
    "\n",
    "The high-degree Polynomial Regression model may severely overfit the training data, while the linear model may underfit it. In general we won’t know what function (what degree polynomial) generated the data, so how can we decide how complex our model should be? How can we tell that our model is overfitting or underfitting the data?\n",
    "\n",
    "## How do we choose the optimal degree of the polynomial?\n",
    "\n",
    "We vary the degree of the polynomial and train the Linear Regression model using the training data. Then, compute the mean squared error (mse) for the test data using the models with varying degree.\n",
    "\n",
    "Finally, we plot the root mean square error (rmse) values against the varying degree. From this plot we find the optimal degree (that gives the smallest mse).\n",
    "\n",
    "\n",
    "## <font color=red> Note for Assignment:\n",
    "\n",
    "For the assignment you will have to use cross-validation to get an estimate of a polynomial model’s generalization performance. \n",
    "\n",
    "You will write a function to plot the training and validation root mean square error values (rmse) for the data matrix X with polynomial degree starting from 1 up to a max. polynomial degree. It takes the feature matrix X (usually the training data) and the max polynomial degree; and by using cross-validation computes the average mse values for the training data and the validation data for each degree polynomial X between 1 and the degree set by max polynomial degree. For example, if max polynomial degree = 3, then it will iterate 3 times and in each iteration it will use the data matrix X with polynomial degree 1, 2 and 3, respectively. During each iteration it will use cross-validation to create the training fold and the validation fold. Then, it will compute the average mse values for both the training and the validation fold. Finally, the function will plot the root-mean-square error (rmse) values for the training and validation folds for each degree of the data matrix starting from 1 up to the max polynomial degree.\n",
    "\n",
    "\n",
    "## Model's Complexity: Overfitting or Underfitting\n",
    "If a model performs well on the training data but generalizes poorly (on the validation data) according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Optimal Degree of the Polynomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5ZPHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "degreeList = [1,2,3]\n",
    "\n",
    "mse_train, mse_test = [], []\n",
    "\n",
    "for degree in degreeList:\n",
    "\n",
    "    model = make_pipeline(PolynomialFeatures(degree, include_bias=False), StandardScaler(), LinearRegression()) \n",
    "  \n",
    "    model.fit(X_train, y_train)\n",
    "       \n",
    "    # Make prediction \n",
    "    y_train_predicted = model.predict(X_train)\n",
    "    y_test_predicted = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    mse_train.append(mean_squared_error(y_train, y_train_predicted))\n",
    "    mse_test.append(mean_squared_error(y_test, y_test_predicted))\n",
    "   \n",
    "\n",
    "plt.figure(figsize=(10, 6))   \n",
    "plt.scatter(degreeList, np.sqrt(mse_test), alpha=1.0, c=\"red\", edgecolors='none', s=50, label=\"Test RMSE\")\n",
    "plt.scatter(degreeList, np.sqrt(mse_train), alpha=1.0, c=\"blue\", edgecolors='none', s=50, label=\"Train RMSE\")    \n",
    "plt.legend(loc=\"best\", fontsize=14) \n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"RMSE for Varying Degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Regularized Polynomial Linear Regression Model using the Optimal Degree\n",
    "\n",
    "\n",
    "A model with some regularization typically performs better than a model without any regularization. Thus, we should generally prefer regularized Regression over plain Linear Regression.\n",
    "\n",
    "Moreover, the Normal Equation requires computing the inverse of a matrix, but that matrix is not always invertible. In contrast, the matrix for regularized Regression (e.g., Ridge Regression) is always invertible.\n",
    "\n",
    " We will now look at three different regularization OLS models.\n",
    "\n",
    "- Ridge Regression ($l_2$ norm)\n",
    "- Lasso Regression ($l_1$ norm)\n",
    "- Elastic Net (it combines $l_1$ and $l_2$ priors as regularizer)\n",
    "\n",
    "We will use the OLS Ridge Regression algorithm on the high-degree polynomial data matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "We will impleent the Ridge Regression with Scikit-Learn using a closed-form solution. It uses a matrix factorization technique by André-Louis **Cholesky**.\n",
    "\n",
    "We need to set the following two parameters of the Ridge regression model.\n",
    "\n",
    "- alpha : {float, array-like}, shape (n_targets)\n",
    "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization.\n",
    "\n",
    "- solver : {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}\n",
    "\n",
    "Three relevant solvers:\n",
    "1. ‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution.\n",
    "2. ‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set tol and max_iter).\n",
    "3. ‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 5.12\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "\n",
    "# Variable that speficies the degree of the polynomial to be added to the feature vector\n",
    "poly_degree = 2\n",
    "\n",
    "\n",
    "# Add polynomial terms with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "\n",
    "\n",
    "\n",
    "# Regularization parameter\n",
    "ridge_alpha = 0.0001\n",
    "\n",
    "# Create Ridge linear regression object\n",
    "lin_reg_ridge = Ridge(alpha=ridge_alpha, solver=\"cholesky\")\n",
    "\n",
    "# Train the model\n",
    "lin_reg_ridge.fit(X_train_poly, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_ridge = lin_reg_ridge.predict(X_train_poly)\n",
    "\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_ridge))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Using Test Data\n",
    "\n",
    "We will use the optimal degree for the polynomial to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 104)\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 14.22\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Variable that speficies the degree of the polynomial to be added to the feature vector\n",
    "poly_degree = 2\n",
    "\n",
    "\n",
    "# Add polynomial and bias term with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "\n",
    "print(X_test_poly.shape)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_test_poly_predicted = lin_reg_ridge.predict(X_test_poly)\n",
    "\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_poly_predicted))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f' % r2_score(y_test, y_test_poly_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Note: Regularized Polynomial Regression\n",
    "\n",
    "We observe that only the polynomial (2nd degree) model and regularized polynomial model (Ridge Regression) improve the test data performance significantly better than other models (OLS and Gradient Descent).\n",
    "\n",
    "In short, only by increasing model complexity (higher-degree polynomial) and regularizing its weights, we could improve performance on the test data for the given dataset."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
